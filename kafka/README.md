# Kafka Endâ€‘toâ€‘End Pipeline

A Python pipeline that ingests messages from a source Kafka topic to â€œintermediateâ€ topic of local Kafka cluster, and then writes every message into MongoDB for durable storage.

---

## ğŸ“ Overview

- **Stageâ€¯1:** `mes_producer.py`  
  â€¢ Connects to an external/source Kafka cluster  
  â€¢ Reads from `topics.source`  
  â€¢ Produces each record unchanged into the local `topics.intermediate`

- **Stageâ€¯2:** `mes_consumer.py`  
  â€¢ Connects to the local Kafka cluster  
  â€¢ Reads from `topics.intermediate`  
  â€¢ Parses JSON payloads and inserts documents into MongoDB

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  
- PythonÂ 3.8+ with `confluent_kafka` & `pymongo` libraries  
- A running MongoDB instance  
- Network access (SASL credentials) to source Kafka brokers

---

## ğŸš€ Quick Start

1. **Configure** broker addresses & MongoDB in `config/settings.conf`.
2. **Create network**
   ```bash
   docker network create streaming-network --driver bridge
   ```
3. **Launch Kafka cluster**:
   ```bash
   docker-compose -f setup/kafka/docker-compose.yml up -d
   ```
4. **Run Producer** (in another shell):
```bash
python srcs/mes_producer.py
```
5. **Run Consumer**:
   ```bash
python srcs/mes_consumer.py
```
