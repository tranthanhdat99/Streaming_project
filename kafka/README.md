# Kafka Endâ€‘toâ€‘End Pipeline

A Python pipeline that ingests messages from a source Kafka topic to â€œintermediateâ€ topic of local Kafka cluster, and then writes every message into MongoDB for durable storage.

---

## ğŸ“ Overview

- **Stageâ€¯1:** `mes_producer.py`  
  â€¢ Connects to an external/source Kafka cluster  
  â€¢ Reads from `topics.source`  
  â€¢ Produces each record unchanged into the local `topics.intermediate`

- **Stageâ€¯2:** `mes_consumer.py`  
  â€¢ Connects to the local Kafka cluster  
  â€¢ Reads from `topics.intermediate`  
  â€¢ Parses JSON payloads and inserts documents into MongoDB

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  
- PythonÂ 3.8+ with `confluent_kafka` & `pymongo` libraries  
- A running MongoDB instance  
- Network access (SASL credentials) to source Kafka brokers

---

## ğŸ“ Project Structure
```bash
kafka/
â”œâ”€â”€ setup/kafka/  
â”‚   â””â”€â”€ docker-compose.yml    # Kafka brokers & AKHQ UI  
â”œâ”€â”€ config/  
â”‚   â”œâ”€â”€ settings.conf         # broker URLs, topics, MongoDB, logging  
â”‚   â””â”€â”€ kafka_server_jaas.conf  
â”œâ”€â”€ srcs/  
â”‚   â”œâ”€â”€ mes_producer.py       # Stageâ€¯1: Kafka â†’ intermediate topic  
â”‚   â””â”€â”€ mes_consumer.py       # Stageâ€¯2: intermediate â†’ MongoDB  
â””â”€â”€ utils/  
    â”œâ”€â”€ config.py             # loads settings.conf & env overrides  
    â””â”€â”€ logging_utils.py      # rotatingâ€‘file logger setup
```

## ğŸš€ Quick Start
Run **all commands from the `kafka/` root folder**:

1. **Configure** broker addresses & MongoDB in `config/settings.conf`.
2. **Create network**
   ```bash
   docker network create streaming-network --driver bridge
   ```
4. **Launch Kafka cluster**:
   ```bash
   docker-compose -f setup/kafka/docker-compose.yml up -d
   ```
6. **Run Producer** (in other shells):
   ```bash
   python srcs/mes_producer.py
   ```
5. **Run Consumer** (in other shells):
   ```bash
   python srcs/mes_consumer.py
   ```

