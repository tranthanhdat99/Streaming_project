# Kafka Endâ€‘toâ€‘End Monitor

A Python pipeline that ingests messages from a source Kafka topic to â€œintermediateâ€ topic of local Kafka cluster, and then writes every message into MongoDB for durable storage.

---

## ğŸ“ Overview

- **Stageâ€¯1:** `mes_producer.py`  
  â€¢ Connects to an external/source Kafka cluster  
  â€¢ Reads from `topics.source`  
  â€¢ Produces each record unchanged into the local `topics.intermediate`

- **Stageâ€¯2:** `mes_consumer.py`  
  â€¢ Connects to the local Kafka cluster  
  â€¢ Reads from `topics.intermediate`  
  â€¢ Parses JSON payloads and inserts documents into MongoDB

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  
- PythonÂ 3.8+ with `confluent_kafka` & `pymongo` libraries  
- A running MongoDB instance  
- Network access (SASL credentials) to your source Kafka brokers

---

## ğŸš€ Quick Start

1. **Configure** broker addresses & MongoDB in `config/settings.conf` (see below).  
2. **Launch Kafka cluster**:
   ```bash
   cd kafka/setup/kafka
   docker-compose up -d

