# Spark Streaming Analytics

A realâ€‘time data processing pipeline: Spark Structured Streaming reads customer events from local Kafka topic, applies UDF enrichments & perâ€‘table transforms, and upserts results into PostgreSQL tables.

---

## ğŸ“ Overview

1. **Ingest**  
   Spark reads messages from a Kafka â€œintermediateâ€ topic.

2. **Transform & Enrich**  
   â€¢ JSON schema validation (`utils/schema.py`)  
   â€¢ UDFs (e.g. IPâ†’country lookup in `udf/`)  
   â€¢ Perâ€‘table logic in `tables/` (store, referrer, location, time, OS, browser, product, fact_event)

3. **Write**  
   JDBC upserts into Postgres via `utils/pg_writer.py` with conflict handling from `utils/table_conflict.py`.

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  (or Spark standalone)  
- Javaâ€¯17 (JRE)  
- PostgreSQL instance reachable from Spark  
- Python 3.8+ dependencies (listed in `requirements.txt`)

---

## ğŸ“ Project Structure

```bash
spark/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.conf        # Spark, Kafka & Postgres parameters
â”‚   â””â”€â”€ log4j/properties     # Log4j config
â”œâ”€â”€ setup/spark/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ ip2location/             # IP2LOCATIONâ€‘LITEâ€‘DB3.IPV6.BIN
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ srcs/
â”‚   â””â”€â”€ kafka_streaming.py   # Entry point for Structured Streaming job
â”œâ”€â”€ udf/                     # UDF modules (e.g. IPâ†’country)
â”œâ”€â”€ tables/                  # Tableâ€‘specific transform functions
â””â”€â”€ utils/
    â”œâ”€â”€ config.py            # Loads `config/settings.conf`
    â”œâ”€â”€ logger.py            # Log4j wrapper
    â”œâ”€â”€ pg_writer.py         # JDBC upsert utility
    â”œâ”€â”€ schema.py            # Defines message schema
    â””â”€â”€ table_conflict.py    # ON CONFLICT clauses for upserts
```

---

## ğŸš€ Quick Start

Run **all commands from the `spark/` root folder**:

1. **Create network**
   ```bash
   docker network create streaming-network --driver bridge
   ```
2. **Run Spark**
   2.1. **Build Spark image**  
   ```bash
   docker build -f setup/spark/Dockerfile -t unigap/spark:3.5 .
   ```
   2.2. **Build Spark image**
   ```bash
   docker volume create spark_data
   docker volume create spark_lib
   ```
1. **Build Spark image**  
   ```bash
   docker build -f setup/spark/Dockerfile -t unigap/spark:3.5 .
