# Spark Streaming Analytics

A realâ€‘time data processing pipeline: Spark Structured Streaming reads customer events from local Kafka topic, applies UDF enrichments & perâ€‘table transforms, and upserts results into PostgreSQL tables.

---

## ğŸ“ Overview

1. **Ingest**  
   Spark reads messages from a Kafka â€œintermediateâ€ topic.

2. **Transform & Enrich**  
   â€¢ JSON schema validation (`utils/schema.py`)  
   â€¢ UDFs (e.g. IPâ†’country lookup in `udf/`)  
   â€¢ Perâ€‘table logic in `tables/` (store, referrer, location, time, OS, browser, product, fact_event)

3. **Write**  
   JDBC upserts into Postgres via `utils/pg_writer.py` with conflict handling from `utils/table_conflict.py`.

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  (or Spark standalone)  
- Javaâ€¯17 (JRE)  
- PostgreSQL instance reachable from Spark  
- Python 3.8+ dependencies (listed in `requirements.txt`)

---

## ğŸš€ Quick Start

Run **all commands from the `spark/` root folder**:

1. **Build Spark image**  
   ```bash
   docker build -f setup/spark/Dockerfile -t unigap/spark:3.5 .
