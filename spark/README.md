# Spark Streaming Analytics

A realâ€‘time data processing pipeline: Spark Structured Streaming reads customer events from local Kafka topic, applies UDF enrichments & perâ€‘table transforms, and upserts results into PostgreSQL tables.

---

## ğŸ“ Overview

1. **Ingest**  
   Spark reads messages from a Kafka â€œintermediateâ€ topic.

2. **Transform & Enrich**  
   â€¢ JSON schema validation (`utils/schema.py`)  
   â€¢ UDFs (e.g. IPâ†’country lookup in `udf/`)  
   â€¢ Perâ€‘table logic in `tables/` (store, referrer, location, time, OS, browser, product, fact_event)

3. **Write**  
   JDBC upserts into Postgres via `utils/pg_writer.py` with conflict handling from `utils/table_conflict.py`.

---

## âš™ï¸ Prerequisites

- Docker & Docker Compose  (or Spark standalone)  
- Javaâ€¯17 (JRE)  
- PostgreSQL instance reachable from Spark  
- Python 3.8+ dependencies (listed in `requirements.txt`)

---

## ğŸ“ Project Structure

```bash
spark/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.conf        # Spark, Kafka & Postgres parameters
â”‚   â””â”€â”€ log4j/properties     # Log4j config
â”œâ”€â”€ setup/spark/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ ip2location/             # IP2LOCATIONâ€‘LITEâ€‘DB3.IPV6.BIN
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ srcs/
â”‚   â””â”€â”€ kafka_streaming.py   # Entry point for Structured Streaming job
â”œâ”€â”€ udf/                     # UDF modules (e.g. IPâ†’country)
â”œâ”€â”€ tables/                  # Tableâ€‘specific transform functions
â””â”€â”€ utils/
    â”œâ”€â”€ config.py            # Loads `config/settings.conf`
    â”œâ”€â”€ logger.py            # Log4j wrapper
    â”œâ”€â”€ pg_writer.py         # JDBC upsert utility
    â”œâ”€â”€ schema.py            # Defines message schema
    â””â”€â”€ table_conflict.py    # ON CONFLICT clauses for upserts
```

---

## ğŸš€ Quick Start

Run **all commands from the `spark/` root folder**:

1. **Create network**
   ```bash
   docker network create streaming-network --driver bridge
   ```
2. **Run Spark**
   2.1. **Build Spark image**  
   ```bash
   docker build -f setup/spark/Dockerfile -t unigap/spark:3.5 .
   ```
   2.2. **Create volume**
   ```bash
   docker volume create spark_data
   docker volume create spark_lib
   ```
   2.3. **Launch Spark cluster**
   ```bash
   docker compose -f setup/spark/docker-compose.yml up -d
   ```
3. **Create folder and copy IP2Location file into Spark docker**
   3.1. **Create folder**  
   ```bash
   docker exec -ti spark-spark-1 mkdir -p /data/location
   docker exec -ti spark-spark-worker-1 mkdir -p /data/location
   docker exec -ti spark-spark-worker-2 mkdir -p /data/location
   ```
   3.2. **Copy data** 
   ```bash
   docker cp ip2location/IP2LOCATION-LITE-DB3.IPV6.BIN spark-spark-1:/data/location
   docker cp ip2location/IP2LOCATION-LITE-DB3.IPV6.BIN spark-spark-worker-1:/data/location
   docker cp ip2location/IP2LOCATION-LITE-DB3.IPV6.BIN spark-spark-worker-2:/data/location
   ```
4. **Test connection to Kafka server**  
   ```bash
   telnet HOST1 PORT1
   telnet HOST2 PORT2
   telnet HOST3 PORT3 
   ```
5. **Run program**  
   ```bash
   (cd . && zip -r code.zip utils udf tables) && \
   docker container stop kafka-streaming || true && \
   docker container rm kafka-streaming || true && \
   docker run -ti --name kafka-streaming \
   --network=streaming-network \
   -p 4040:4040 \
   -v ./:/spark \
   -v spark_lib:/opt/bitnami/spark/.ivy2 \
   -v spark_data:/data \
   -e KAFKA_SASL_JAAS_CONFIG='org.apache.kafka.common.security.plain.PlainLoginModule required username="kafka" password="UnigapKafka@2024";' \
   -e PYSPARK_DRIVER_PYTHON='python' \
   -e PYSPARK_PYTHON='./environment/bin/python' \
   unigap/spark:3.5 bash -c "python -m venv pyspark_venv && \
   source pyspark_venv/bin/activate && \
   pip install -r /spark/requirements.txt && \
   venv-pack -o pyspark_venv.tar.gz && \
   spark-submit \
   --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.3 \
   --archives pyspark_venv.tar.gz#environment \
   --py-files /spark/code.zip \
   /spark/srcs/kafka_streaming.py"
   ```

