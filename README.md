
# Realâ€‘Time Customer Insights for Glamira

This project demonstrates a full streaming data pipeline that captures customer interactions on the Glamira website and turns them into structured insights. It uses Apache Kafka to ingest click and view events, Spark Structured Streaming to enrich and transform data, and Airflow to orchestrate and monitor each stage.

---

## âœ¨ Core Capabilities

- **Event Streaming**  
  Capture every product view, addâ€‘toâ€‘cart, and filter action from the Glamira site in real time.

- **Data Enrichment**  
  Apply IPâ€‘toâ€‘location lookups, parse timestamps, and extract key fields via reusable UDFs.

- **Incremental Upserts**  
  Persist aggregated and cleaned records into PostgreSQL tables, handling conflicts gracefully.

- **Automated Monitoring & Alerts**  
  Use Airflow DAGs to run health checks on Kafka brokers, Spark cluster, and Postgres ingestionâ€”notify Slack on any issues.

---

## ğŸ“Š Key Use Cases

1. **Live Product Popularity**  
   Maintain upâ€‘toâ€‘date topâ€‘N lists of most viewed rings, necklaces, and other accessories.

2. **Geographic Trends**  
   Track visitor locations by country and city for targeted marketing and inventory planning.

3. **Store Page Performance**  
   Compare traffic across regional store pages (e.g., USA vs. Germany) to optimize localization.

4. **Operational Reliability**  
   Get immediate alerts if any component (Kafka, Spark, or Postgres) experiences downtime or lag.

---

## âš™ï¸ Architecture

```text
[ Website Events ] 
       â†“ (Kafka Source Topic)
[ Kafka Producer ] â”€â”€â–¶ [ Kafka Intermediate Topic ] â”€â”€â–¶ [ Kafka Consumer ] â”€â”€â–¶ MongoDB
                                                        â†“
                                             [ Spark Structured Streaming ]
                                                        â†“
                                                  PostgreSQL Tables
                                                        â†“
                                               [ Airflow Monitoring DAGs ]
                                                        â†“
                                                  Slack Notifications
```

- **Kafka**  
  Handles event ingestion and decoupling from the source website.

- **MongoDB**  
  Acts as a raw event archive for all ingested messages.

- **Spark**  
  Processes streaming data into analytical tables with UDF enrichments and upserts.

- **Airflow**  
  Orchestrates and monitors each pipeline stage, sending Slack alerts on failures.

---

## ğŸš€ Getting Started

1. **Clone this repository**  
   ```bash
   git clone https://github.com/your-user/your-repo.git
   cd your-repo 
2. **Follow each subfolderâ€™s README** for setup and run instructions:  
   - `kafka/` â€“ streaming and Mongo archiving  
   - `spark/` â€“ lightweight transformations and Postgres upserts  
   - `airflow/` â€“ simple DAGs and alerts  
3. **Install prerequisites**:

     - Docker & Docker Compose
     - PythonÂ 3.8+ (with `confluent_kafka`, `pymongo`)
     - JavaÂ 17 (for Spark)
     - Running instances of MongoDB and PostgreSQL
4. **Configure** connection strings, credentials, and thresholds in each `config/` folder or via environment variables.
5. **Launch services**

    Start components in sequence:
